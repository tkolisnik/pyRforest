% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modelTrainingTuningFittingTesting.R
\name{tune_and_train_rf_model_bayes}
\alias{tune_and_train_rf_model_bayes}
\title{Tune and train RandomForest model using Bayesian optimization}
\usage{
tune_and_train_rf_model_bayes(
  X,
  y,
  cv_folds = 5,
  scoring_method = "roc_auc",
  seed = 4,
  param_grid = NULL,
  n_jobs = 1,
  n_cores = -2
)
}
\arguments{
\item{X}{Training features (data frame or matrix). Typically obtained from the \code{create_feature_matrix} function.}

\item{y}{Target vector for the model. Usually obtained from the \code{create_feature_matrix} function.}

\item{cv_folds}{The number of cross-validation splits in \code{StratifiedKFold} (default: 5).}

\item{scoring_method}{The scoring method to be used during optimization (default: 'roc_auc'). Options include 'accuracy', 'precision', 'recall', 'f1', etc. See scikit-learn's \code{BayesSearchCV} documentation for a full list of scoring methods.}

\item{seed}{An optional random seed for reproducibility (default: 4).}

\item{param_grid}{An optional list of hyperparameters for Bayesian optimization.
If NULL, a default grid will be used. The list should follow the format expected by \code{BayesSearchCV}.
The default \code{param_grid} includes:
\itemize{
\item \code{bootstrap} (list): \link{TRUE}
\item \code{class_weight} (list): \link{NULL}
\item \code{max_depth} (list): \link{5L, 10L, 15L, 20L, NULL}
\item \code{n_estimators} (integer sequence): \link{50, 75, 100, 125, 150, 175, 200}
\item \code{max_features} (list): \link{"sqrt", "log2", 0.1, 0.2}
\item \code{criterion} (list): \link{"gini"}
\item \code{warm_start} (list): \link{FALSE}
\item \code{min_samples_leaf} (list): \link{2L, 3L, 4L, 5L, 10L}
\item \code{min_samples_split} (list): \link{2L, 3L, 4L, 5L, 10L}
}}

\item{n_jobs}{The number of jobs to run in parallel during training. Default is 1.}

\item{n_cores}{The number of cores to use for parallel processing. Default is -2, meaning it will use all but 2 cores.}
}
\value{
A list containing the best hyperparameters for the model, the cross-validation score, the trained \code{BayesSearchCV} object, and memory usage statistics.
}
\description{
This function uses scikit-learn's python-based \code{BayesSearchCV} from the \code{skopt} library
to perform hyperparameter tuning and training of a \code{RandomForestClassifier}.
It leverages Bayesian optimization for a more efficient hyperparameter search,
allowing more optimal parameter configurations to be discovered faster and with less compute time when compared to brute-force methods like GridSearchCV.
It supports binary and categorical multiclass classification, and the function includes preprocessing steps of one-hot encoding and scaling for compatibility with the random forest algorithm.
Please reference the scikit-learn \code{BayesSearchCV} documentation for more details on customization.
}
\examples{
\dontshow{if (FALSE) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
library(pyRforest)
library(reticulate)

# Load the conda environment for the package dependencies
use_condaenv("pyRforest-conda", conda = "conda path see vignette", required = TRUE)

# Load sample data
 data(demo_rnaseq_data)

# Prepare the sample data into a format ingestible by the ML algorithm
processed_training_data <- pyRforest::create_feature_matrix(dataset = demo_rnaseq_data$training_data,
    set_type = "training")

# Model training and tuning
tuning_results <- pyRforest::tune_and_train_rf_model_bayes(
    X = processed_training_data$X_training_mat,
    y = processed_training_data$y_training_vector,
    cv_folds = 5,
    scoring = 'roc_auc', # use 'roc_auc_ovr' for multiclass targets
     seed = 4,
    n_jobs = 1,
    n_cores = -2)
print(tuning_results$best_params)
print(tuning_results$best_score)
\dontshow{\}) # examplesIf}
}
