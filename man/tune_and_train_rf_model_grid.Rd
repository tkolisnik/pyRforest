% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modelTrainingTuningFittingTesting.R
\name{tune_and_train_rf_model_grid}
\alias{tune_and_train_rf_model_grid}
\title{Perform hyperparameter tuning and training for RandomForestClassifier}
\usage{
tune_and_train_rf_model_grid(
  X,
  y,
  cv_folds = 5,
  scoring_method = "roc_auc",
  seed = 4,
  param_grid = NULL,
  n_jobs = 1,
  n_cores = -2
)
}
\arguments{
\item{X}{The features for the model (data frame or matrix). Usually obtained from the create_feature_matrix function.}

\item{y}{The target variable for the model (vector). Usually obtained from the create_feature_matrix function.}

\item{cv_folds}{The number of splits in StratifiedKFold cross validation, (default: 5)}

\item{scoring_method}{The scoring method to be used. Options are 'accuracy', 'precision', 'recall', 'roc_auc', 'f1'... see scikit-learn GridSearchCV documentation for more info.}

\item{seed}{The random seed for reproducibility (default: 4).}

\item{param_grid}{An optional list of parameters for tuning the model.
If NULL, a default set of parameters is used. The list should follow
the format expected by GridSearchCV, with parameters requiring integers
suffixed with 'L' (e.g., 10L). This is to ensure compatibility when being passed from R to Python.
Default param_grid is as follows:
param_grid <- list(
bootstrap = list(TRUE),
class_weight = list(NULL),
max_depth = list(5L, 10L, 15L, 20L, NULL),
n_estimators = as.integer(seq(10, 100, 10)),
max_features = list("sqrt", "log2", 0.1, 0.2),
criterion = list("gini"),
warm_start = list(FALSE),
min_samples_leaf = list(1L, 2L, 5L, 10L, 20L, 50L),
min_samples_split = list(2L, 10L, 20L, 50L, 100L, 200L)
)}

\item{n_jobs}{An optional number of jobs to specify for parallel processing. Default is 1.}

\item{n_cores}{An optional number of cores to specify for parallel processing. Default is (-2), which is 2 less than the maximum available number of cores.}
}
\value{
A list containing the best hyperparameters for the model, cross-validation scores on training set,
and the fitted GridSearchCV object.
}
\description{
This function uses scikit-learn's python based GridSearchCV to perform hyperparameter tuning and training of a RandomForestClassifier.
It supports binary and categorical multiclass classification and it allows for customizable parameter grids and includes preprocessing steps of one-hot encoding
and scaling. The function is designed to find the best hyperparameters using a brute force search. Please reference the scikit-learn GridSearchCV documentation for the full description of options, however our defaults are comprehensive.
}
\examples{
\dontshow{if (FALSE) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
library(pyRforest)
library(reticulate)

# Load conda environment, which ensures the correct version of Python and the necessary python packages can be loaded. See vignette for more details.
use_condaenv("pyRforest-conda", conda = "conda path see vignette", required = TRUE)

# Load the demo data
data(demo_rnaseq_data)

# Prepare the sample data into a format ingestible by the ML algorithm
processed_training_data <- pyRforest::create_feature_matrix(dataset = demo_rnaseq_data$training_data,
    set_type = "training")

# Model tuning and training (Warning: may take a long time if dataset is large and if param_grid has many options)
tuning_results <- pyRforest::tune_and_train_rf_model_grid(
    X = processed_training_data$X_training_mat,
    y = processed_training_data$y_training_vector,
    cv_folds = 5,
   scoring = 'roc_auc', #use 'roc_auc_ovr' for multiclass targets
    seed = 123,
    param_grid = custom_parameter_grid,
    n_jobs = 1,
    n_cores = -2)
print(tuning_results$best_params)
print(tuning_results$best_score_)
\dontshow{\}) # examplesIf}
}
